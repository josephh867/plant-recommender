{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca81afd2-3f24-4e2c-93ac-3ac042947edf",
   "metadata": {},
   "source": [
    "# Plant Recommender Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a792dfb2-a0d7-40df-903e-0af832718344",
   "metadata": {},
   "source": [
    "## Cluster Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d26407-965e-4da8-abd1-35859741f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans, DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe30ce-9511-46bc-9bf3-b0ed99e0d6c4",
   "metadata": {},
   "source": [
    "#### DBSCAN\n",
    "\n",
    "Clustering plays an important part in this project - it will provide the basis for the suggestion engine later. To create the best possible clustering model, I will need to have a rough estimate of the number of datapoint clusters. I don't intuitively know the number of clusters necesary since the data has a high degree of dimensionality, so to estimate it I will utilize `DBSCAN` clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6044966a-2ea2-42e8-ae66-d24ebbd2a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data and scale it\n",
    "df = pd.read_csv('../datasets/cleaned-data.csv')\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bebfba7c-a0e5-4552-b9c9-aa5091eeeb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['id', 'Scientific_Name_x'])\n",
    "species = df[['id', 'Scientific_Name_x']]\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_sc = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd52a341-4ada-4767-9b47-ccde1dd39d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdce675d-d55a-4695-b306-ad45c078d65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(eps=10, min_samples=2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a DBSCAN model\n",
    "db = DBSCAN(eps=10, min_samples=2)\n",
    "db.fit(X_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2129b15-4465-41bf-a43b-2b1bdbe823e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of clusters and look at the silouette score\n",
    "labels = db.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91a458cd-b1d3-4584-b440-d4feb3c48035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1460248480136029"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(X_sc, db.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80286c7c-d196-4b6a-8288-6605d93cf944",
   "metadata": {},
   "source": [
    "This score is not great, but that doesn't particularly matter, as this model was instead meant to give a best estimate on the number of clusters needed to represent the data well. The `n_clusters_` of about 70 found here will be indispensible for testing other clustering models, such as KMeans.\n",
    "\n",
    "#### KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5a5379-cc94-422c-a5bd-b71181f408b6",
   "metadata": {},
   "source": [
    "To see if this silouette score can be improved upon, let's try to use a KMeans clustering model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92cb2458-b9fb-43d8-89b2-da7876430c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=70, random_state=42)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters=70, random_state=42)\n",
    "km.fit(X_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fc5bea6-dc63-410c-be3a-ee3c76f77fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.049270928938044745"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(X_sc, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f30e84-23bf-48f3-97f7-f453f40eaaba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
